{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kohya's Script + Lora Train `Developing`\n",
    "\n",
    "More info: https://github.com/wibus-wee/stable_diffusion_chilloutmix_ipynb\n",
    "\n",
    "> Created by [@wibus-wee](https://github.com/wibus-wee)\n",
    ">\n",
    "> Reference: [camenduru/stable-diffusion-webui-colab](https://github.com/camenduru/stable-diffusion-webui-colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 选择模型 { display-mode: \"form\" }\n",
    "\n",
    "#@markdown 选择模型\n",
    "checkpoint = 'chilloutmix.safetensors' #@param [\"chilloutmix.safetensors\", \"sunshinemix.safetensors\", \"grapefruitHentaiModel.safetensors\"]\n",
    "\n",
    "downloadLink = {\n",
    "    'chilloutmix.safetensors': 'https://civitai.com/api/download/models/11745',\n",
    "    'sunshinemix.safetensors': 'https://civitai.com/api/download/models/11752',\n",
    "    'grapefruitHentaiModel.safetensors': 'https://civitai.com/api/download/models/9000'\n",
    "}[checkpoint]\n",
    "\n",
    "print(\"已选择模型: \" + checkpoint + \" <===> \" + downloadLink)\n",
    "\n",
    "%store checkpoint\n",
    "%store downloadLink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 1. 检查 GPU & 检查环境\n",
    "\n",
    "#@markdown 此步骤将检查你的 GPU 是否支持 xformers，同时会检查是否为 Paperspace 平台（仅针对 M4000 GPU 做了判断，因此其他的 GPU 可能在此处将会出现逻辑判断错误的情况，你可能需要自行勾选）。\n",
    "\n",
    "import os, subprocess\n",
    "paperspace_m4000 = False\n",
    "#@markdown 是否为 Paperspace 平台\n",
    "isPaperspace = False #@param {type:\"boolean\"}\n",
    "try:\n",
    "    subprocess.run(['nvidia-smi', '--query-gpu=name', '--format=csv,noheader'], stdout=subprocess.PIPE)\n",
    "    if 'M4000' in subprocess.run(['nvidia-smi', '--query-gpu=name', '--format=csv,noheader'], stdout=subprocess.PIPE).stdout.decode('utf-8'):\n",
    "        print(\"WARNING: 你正在使用的是 Quadro M4000 GPU，它将无法使用 xformers。\")\n",
    "        paperspace_m4000 = True\n",
    "        isPaperspace = True\n",
    "    else:\n",
    "        print(\"你正在使用的是合适的 GPU - \" + subprocess.run(['nvidia-smi', '--query-gpu=name', '--format=csv,noheader'], stdout=subprocess.PIPE).stdout.decode('utf-8') + \"。\")\n",
    "        print(\"平台: Paperspace\" if isPaperspace else \"使用平台: Colab\")\n",
    "except:\n",
    "    print(\"似乎没有 GPU 可用。请检查你的运行时类型。\")\n",
    "    exit()\n",
    "\n",
    "rootDir = isPaperspace and '/tmp' or '/content'\n",
    "%store rootDir\n",
    "%store paperspace_m4000 \n",
    "%store isPaperspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 2. 安装训练依赖\n",
    "\n",
    "%store -r rootDir \n",
    "%store -r checkpoint \n",
    "%store -r downloadLink\n",
    "\n",
    "!apt-get -y install -qq aria2\n",
    "ariaInstalled = False\n",
    "\n",
    "try:\n",
    "    subprocess.run(['aria2c', '--version'], stdout=subprocess.PIPE)\n",
    "    ariaInstalled = True\n",
    "except:\n",
    "    pass\n",
    "\n",
    "!git clone --recurse-submodules https://github.com/Akegarasu/lora-scripts {rootDir}/lora-scripts\n",
    "\n",
    "%cd {rootDir}/lora-scripts\n",
    "if ariaInstalled:\n",
    "    !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M {downloadLink} -d {rootDir}/lora-scripts/sd-models -o {checkpoint}\n",
    "else:\n",
    "    !wget -c {downloadLink} -P {rootDir}/stable-diffusion-webui/models/Stable-diffusion -O {rootDir}/lora-scripts/sd-models/{checkpoint}\n",
    "!chmod +x install.bash\n",
    "!bash install.bash\n",
    "\n",
    "if not paperspace_m4000:\n",
    "  !pip install -q https://github.com/camenduru/stable-diffusion-webui-colab/releases/download/0.0.16/xformers-0.0.16+814314d.d20230118-cp38-cp38-linux_x86_64.whl\n",
    "  !pip install -q --pre triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 3. 训练选项设置\n",
    "%store -r rootDir \n",
    "%store -r checkpoint \n",
    "\n",
    "!source venv/bin/activate\n",
    "\n",
    "#@markdown 底膜路径 - 已跟随 Checkpoint 设置\n",
    "pretrained_model = rootDir + \"/lora-scripts/sd-models/\" + checkpoint\n",
    "#@markdown 训练数据集路径\n",
    "train_data_dir = \"./train/aki\" #@param {type:\"string\"}\n",
    "#@markdown 图片分辨率，宽,高。支持非正方形，但必须是 64 倍数。\n",
    "resolution = \"512,512\" #@param [\"256,256\", \"512,512\", \"1024,1024\", \"2048,2048\"]\n",
    "#@markdown batch size | 批大小\n",
    "batch_size = 1 #@param {type:\"number\"}\n",
    "#@markdown max train epoches | 最大训练 epoch\n",
    "max_train_epoches = 10 #@param {type:\"number\"}\n",
    "#@markdown save every n epochs | 每 N 个 epoch 保存一次\n",
    "save_every_n_epochs = 2 #@param {type:\"number\"}\n",
    "#@markdown network dim | 常用 4~128，不是越大越好\n",
    "network_dim = 32 #@param {type:\"number\"}\n",
    "#@markdown network alpha | 常用与 network_dim 相同的值或者采用较小的值，如 network_dim的一半 防止下溢。默认值为 1，使用较小的 alpha 需要提升学习率。\n",
    "network_alpha = 32 #@param {type:\"number\"}\n",
    "#@markdown clip skip | 玄学 一般用 2\n",
    "clip_skip = 2 #@param {type:\"number\"}\n",
    "#@markdown train U-Net only | 仅训练 U-Net，开启这个会牺牲效果大幅减少显存使用。6G显存可以开启\n",
    "train_unet_only = False #@param {type:\"boolean\"}\n",
    "#@markdown train Text Encoder only | 仅训练 文本编码器\n",
    "train_text_encoder_only = False #@param {type:\"boolean\"}\n",
    "#@markdown 学习率\n",
    "lr = \"1e-4\" #@param {type:\"string\"}\n",
    "#@markdown unet_lr | U-Net 学习率\n",
    "unet_lr = \"1e-4\" #@param {type:\"string\"}\n",
    "#@markdown text_encoder_lr | 文本编码器学习率\n",
    "text_encoder_lr = \"1e-5\" #@param {type:\"string\"}\n",
    "#@markdown lr_scheduler | \"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\", \"constant\", \"constant_with_warmup\"\n",
    "lr_scheduler = \"cosine_with_restarts\" #@param [\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\", \"constant\", \"constant_with_warmup\"] {type:\"string\"}\n",
    "#@markdown lr_warmup_steps | 仅在 lr_scheduler 为 constant_with_warmup 时需要填写这个值\n",
    "lr_warmup_steps = 0 #@param {type:\"number\"}\n",
    "#@markdown output model name | 模型保存名称\n",
    "output_name = \"aki\" #@param {type:\"string\"}\n",
    "#@markdown model save ext | 模型保存格式 ckpt, pt, safetensors\n",
    "save_model_as = \"safetensors\" #@param [\"ckpt\", \"pt\", \"safetensors\"] {type:\"string\"}\n",
    "#@markdown 若需要从已有的 LoRA 模型上继续训练，请填写 LoRA 模型路径。\n",
    "network_weights = \"\" #@param {type:\"string\"}\n",
    "#@markdown arb min resolution | arb 最小分辨率\n",
    "min_bucket_reso = 256 #@param {type:\"number\"}\n",
    "#@markdown arb max resolution | arb 最大分辨率\n",
    "max_bucket_reso = 1024 #@param {type:\"number\"}\n",
    "#@markdown persistent dataloader workers | 容易爆内存，保留加载训练集的worker，减少每个 epoch 之间的停顿\n",
    "persistent_data_loader_workers = 0 #@param {type:\"number\"}\n",
    "#@markdown use 8bit adam optimizer | 使用 8bit adam 优化器节省显存，默认启用。部分 10 系老显卡无法使用，修改为 0 禁用。\n",
    "use_8bit_adam = True #@param {type:\"boolean\"}\n",
    "#@markdown use lion optimizer | 使用 Lion 优化器\n",
    "use_lion = False #@param {type:\"boolean\"}\n",
    "\n",
    "!echo \"\" > _train.sh # clear\n",
    "!echo \"pretrained_model={pretrained_model}\" >> _train.sh\n",
    "!echo \"train_data_dir={train_data_dir}\" >> _train.sh\n",
    "!echo \"resolution={resolution}\" >> _train.sh\n",
    "!echo \"batch_size={batch_size}\" >> _train.sh\n",
    "!echo \"max_train_epoches={max_train_epoches}\" >> _train.sh\n",
    "!echo \"save_every_n_epochs={save_every_n_epochs}\" >> _train.sh\n",
    "!echo \"network_dim={network_dim}\" >> _train.sh\n",
    "!echo \"network_alpha={network_alpha}\" >> _train.sh\n",
    "!echo \"clip_skip={clip_skip}\" >> _train.sh\n",
    "!echo \"train_unet_only={train_unet_only}\" >> _train.sh\n",
    "!echo \"train_text_encoder_only={train_text_encoder_only}\" >> _train.sh\n",
    "!echo \"lr={lr}\" >> _train.sh\n",
    "!echo \"unet_lr={unet_lr}\" >> _train.sh\n",
    "!echo \"text_encoder_lr={text_encoder_lr}\" >> _train.sh\n",
    "!echo \"lr_scheduler={lr_scheduler}\" >> _train.sh\n",
    "!echo \"lr_warmup_steps={lr_warmup_steps}\" >> _train.sh\n",
    "!echo \"output_name={output_name}\" >> _train.sh\n",
    "!echo \"save_model_as={save_model_as}\" >> _train.sh\n",
    "!echo \"network_weights={network_weights}\" >> _train.sh\n",
    "!echo \"min_bucket_reso={min_bucket_reso}\" >> _train.sh\n",
    "!echo \"max_bucket_reso={max_bucket_reso}\" >> _train.sh\n",
    "!echo \"persistent_data_loader_workers={persistent_data_loader_workers}\" >> _train.sh\n",
    "!echo \"use_8bit_adam={use_8bit_adam}\" >> _train.sh\n",
    "!echo \"use_lion={use_lion}\" >> _train.sh\n",
    "\n",
    "NO_NOT_MODIFY_CONTENTS = \"\"\"\n",
    "# ============= DO NOT MODIFY CONTENTS BELOW | 请勿修改下方内容 =====================\n",
    "export HF_HOME=\"huggingface\"\n",
    "export TF_CPP_MIN_LOG_LEVEL=3\n",
    "extArgs=()\n",
    "if [ $train_unet_only == 1 ]; then extArgs+=(\"--network_train_unet_only\"); fi\n",
    "if [ $train_text_encoder_only == 1 ]; then extArgs+=(\"--network_train_text_encoder_only\"); fi\n",
    "if [ $network_weights ]; then extArgs+=(\"--network_weights $network_weights\"); fi\n",
    "if [ $use_8bit_adam == 1 ]; then extArgs+=(\"--use_8bit_adam\"); fi\n",
    "if [ $use_lion == 1 ]; then extArgs+=(\"--use_lion_optimizer\"); fi\n",
    "if [ $persistent_data_loader_workers == 1 ]; then extArgs+=(\"--persistent_data_loader_workers\"); fi\n",
    "accelerate launch --num_cpu_threads_per_process=8 \"./sd-scripts/train_network.py\" \\\n",
    "  --enable_bucket \\\n",
    "  --pretrained_model_name_or_path=$pretrained_model \\\n",
    "  --train_data_dir=$train_data_dir \\\n",
    "  --output_dir=\"./output\" \\\n",
    "  --logging_dir=\"./logs\" \\\n",
    "  --resolution=$resolution \\\n",
    "  --network_module=networks.lora \\\n",
    "  --max_train_epochs=$max_train_epoches \\\n",
    "  --learning_rate=$lr \\\n",
    "  --unet_lr=$unet_lr \\\n",
    "  --text_encoder_lr=$text_encoder_lr \\\n",
    "  --lr_scheduler=$lr_scheduler \\\n",
    "  --lr_warmup_steps=$lr_warmup_steps \\\n",
    "  --network_dim=$network_dim \\\n",
    "  --network_alpha=$network_alpha \\\n",
    "  --output_name=$output_name \\\n",
    "  --train_batch_size=$batch_size \\\n",
    "  --save_every_n_epochs=$save_every_n_epochs \\\n",
    "  --mixed_precision=\"fp16\" \\\n",
    "  --save_precision=\"fp16\" \\\n",
    "  --seed=\"1337\" \\\n",
    "  --cache_latents \\\n",
    "  --clip_skip=$clip_skip \\\n",
    "  --prior_loss_weight=1 \\\n",
    "  --max_token_length=225 \\\n",
    "  --caption_extension=\".txt\" \\\n",
    "  --save_model_as=$save_model_as \\\n",
    "  --min_bucket_reso=$min_bucket_reso \\\n",
    "  --max_bucket_reso=$max_bucket_reso \\\n",
    "  --xformers --shuffle_caption ${extArgs[@]}\n",
    "\"\"\"\n",
    "\n",
    "!echo \"$NO_NOT_MODIFY_CONTENTS\" >> _train.sh\n",
    "\n",
    "!echo \"Finished writing _train.sh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 4. 开始训练\n",
    "!chmod +x _train.sh\n",
    "!sh _train.sh"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
